---
title: "Lab 3"
author: "Senan Hogan-H."
header-includes:
   - \usepackage{amsmath}
   - \usepackage{hyperref}
date: "8 March 2018"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
gc()
ls()
options(digits=4)
require(tidyverse)
set.seed(47)
```

## Question 1.

a. Define $s^2=\frac{1}{n}\sum(x_i-\bar{x})^2$.
\begin{multline} \\
\sum_{i=1}^n(x_i-\mu)^2 = \\
\sum_{i=1}^n( x_i^2 - 2\mu x_i +\mu^2) = \\
\sum_{i=1}^n x_i^2 - 2 \mu\sum_{i=1}^nx_i + n \mu^2 = \\
\sum_{i=1}^n x_i^2 - 2n \bar{x} ^2 + n \bar{x} ^2 + n\bar{x}^2 - 2 \mu\sum_{i=1}^nx_i + n \mu^2 = \\
\sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^nx_i + n \bar{x} ^2 + n\bar{x}^2 - 2 \mu\sum_{i=1}^nx_i + n \mu^2 = \\
\sum_{i=1}^n(x_i^2 -2\bar{x}x_i+\bar{x}^2) + n(\bar{x}^2 - 2\mu\bar{x} + \mu^2) = \\
\sum_{i=1}^n (x_i-\bar{x})^2  + n(\bar{x}-\mu)^2 = \\
ns^2 + n(\bar{x}-\mu)^2 \\
\end{multline}

b. Start with a normal prior, $f(\mu)=\mathcal{N}(\mu_0,\sigma_0^2)$.

c. Start with a normal prior, $f(\mu)=\mathcal{N}(\mu_0,\sigma_0^2)$, where $\mu_0=\sigma_0=1$.  Suppose that the real distribution has a mean and standard deviation of 3, so that $\bar{x}=3$.
```{r, fig.height = 4, fig.width = 8, fig.align='center'}
x_bar <- sd <- 3
mu_0 <- 1
sd_0 <- 1

prob <- seq(0, 6, .01)
# First prior
x_0 <- dnorm(prob, mu_0, sd_0)

# Update with data sample of 10
n <- 10

sd_10 <- ((n/(sd^2) + 1/(sd_0^2)))^(-0.5)
mu_10 <- sd_10^2*((mu_0/(sd_0^2)) + (n*x_bar)/sd^2)

x_10 <- dnorm(prob, mu_10, sd_10)

# Update with another data sample of 100
n <- 100

sd_100 <- ((n/(sd^2) + 1/(sd_0^2)))^(-0.5)
mu_100 <- sd_100^2*((mu_0/(sd_0^2)) + (n*x_bar)/sd^2)

x_100 <- dnorm(prob, mu_100, sd_100)

data.frame(x_0, x_10, x_100, prob) %>% ggplot(aes(x= prob)) +
  geom_line(aes(y= x_0, colour = 'Prior Distribution')) +
  geom_line(aes(y= x_10, colour = 'Posterior Distrubtion after n=10 sample')) +
  geom_line(aes(y= x_100, colour = 'Posterior Distrubtion after n=100 sample')) +
  labs(title = "Density for Normal, mean of 3, Updated with two Samples",
       x = "mu", y = "Density") + theme_classic()
```

d.  Use a prior of the form $\mathcal{G}(\alpha_0, \beta_0)$ where $\alpha_0, \beta_0 > 0$ to estimate $\lambda$.

```{r, fig.height = 4, fig.width = 8, fig.align='center'}
prob <- seq(0, 5, .01)

alpha_0 <- 1
beta_0 <- 1
x_0 <- dgamma(prob, alpha_0, beta_0)

data.frame(x_0, prob) %>% 
  ggplot(aes(x = prob)) +
  geom_line(aes(y = x_0, colour = 'alpha = beta = 1')) +
  labs(title = "Various Gamma Distributions, Used as Prior for Lambda",
       x = "Lambda", y = "Density")
```

e. THIS QUESTION IS ACTUALLY NOT POSSIBLE WITH GIVEN PRIOR.  CHANGE PROBABILITY TO 0.95.

For the prior, want $\sigma^2=4$, $P(\sigma^2 > 2 )=0.95$.

So that $\frac{\alpha_0 - 1}{\beta_0} = 4$, $pbeta(2, \alpha_0, \beta_0) = 1- 0.95 = 0.05$
```{r}
x <- 2

vector <- seq(1, 3, by = 0.00001)
i <- 1

# (alpha_0 - 1)/beta_0 = 4, so beta_0 = (alpha_0 - 1)/4
alpha_0 <- vector[i]
beta_0 <- (alpha_0 - 1)/4

epsilon <- 0.0001
while (abs(pgamma(x, alpha_0, beta_0) - 0.05) > epsilon){
  #print(i)
  i <- i + 1
  alpha_0 <- vector[i]
  beta_0 <- (alpha_0 - 1)/4
}
rm(vector, i)
alpha_0
beta_0
pgamma(x, alpha_0, beta_0) # roughly = 0.05
```

f. 
```{r}
Normal_estimator <- function(x.data, mu_0, kappa_0, alpha_0, beta_0){
  # returning posterior mean for mu.  First define the prior hyperparameters.
  n <- length(x.data)
  x_bar <- mean(x.data)
  s <- sd(x.data)

  # Then define some posterior hyperparameters.
  mu_n <- (kappa_0*mu_0 + n*x_bar)/(kappa_0 + n)
  kappa_n <- kappa_0 + n
  alpha_n <- alpha_0 + n/2
  beta_n <- beta_0 + (n/2)*s^2 + (kappa_0*n*(x_bar - mu_0)^2)/(2*(kappa_0 + n))

  # Sample from posterior for lambda_star
  lambda.data <- c()
  for (i in c(1:10000)){
    lambda.data <- c(lambda.data,
                 rgamma(1, alpha_n, beta_n))
  }
  lambda_star <- mean(lambda.data)

  # sample fron posterior for mu_star
  mu.data <- c()
  for (i in c(1:10000)){
    mu.data <- c(mu.data,
                 rnorm(1, mu_n, (kappa_n*lambda.data[i])))
  }

  # posterior mean
  mu_star <- mean(mu.data)

  # 95% CI for mu
  mu_CI <- quantile(mu.data, probs = c(0.025, 0.975))

  # 95% CI for new observation
  N <- 100
  new.data <- rnorm(N, mu_star, ((kappa_0 + N)*lambda_star ))
  new_CI <- quantile(new.data, probs = c(0.025, 0.975))
  return(list(mu_star, mu_CI, new_CI))
}

data <- rnorm(10, 12, 1)
Normal_estimator(data , 12, 1, 6, 4)

```

g.
```{r}
data(mtcars)
mpg = mtcars[,1]
missingObs = sample(seq(1,32,1), 1)
mpg_minus = mpg[-missingObs]
missing = mpg[missingObs]
estimateMPG = Normal_estimator(mpg_minus, 20, 1, 30, 30)
estimateMPG[3]
missing
```



\vspace{.5in}Consulted the following resource in working on this exercise: \url{https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf}